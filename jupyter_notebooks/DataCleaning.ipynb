{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(DATA CLEANING)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Handle/Evaluate missing data.\n",
        "* Cleaning data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/HousePrices.csv \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned data in outputs/datasets/cleaned \n",
        " \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* Handle missing data and droping variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading collected data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePrices.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to check the shapes, length and distrubution on the data when cleaning it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(vars_with_missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[vars_with_missing_data].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code from walkthrough project 02\n",
        "from pandas_profiling import ProfileReport\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"No variables has missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def AssessMissingValues(df):\n",
        "    \"\"\"\n",
        "    This function assesses the presence of missing values in a DataFrame.\n",
        "    \"\"\"\n",
        "    # Calculate the absolute number of missing values per column\n",
        "    total_missing = df.isna().sum()\n",
        "\n",
        "    # Calculate the percentage of missing values relative to the total rows\n",
        "    percent_missing = (total_missing / len(df) * 100).round(2)\n",
        "\n",
        "    # Create a new DataFrame to display missing data stats\n",
        "    missing_stats = pd.DataFrame({\n",
        "        'MissingValuesCount': total_missing,\n",
        "        'PercentOfTotal': percent_missing,\n",
        "        'ColumnType': df.dtypes\n",
        "    }).sort_values(by='PercentOfTotal', ascending=False)\n",
        "\n",
        "    # Filter to include only columns with missing values\n",
        "    missing_stats = missing_stats[missing_stats['PercentOfTotal'] > 0]\n",
        "\n",
        "    return missing_stats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AssessMissingValues(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can drop the variables with data that wont have big inpact on the predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handle Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inpiered off code insitutes learning material and https://github.com/Amareteklay/heritage-housing-issues/blob/main/jupyter_notebooks/02%20-%20Data_Cleaning.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "def VisualizeDataCleaningImpact(df_original, df_cleaned, applied_methods_vars):\n",
        "    flag_count = 1  # To keep track of the plot sequence\n",
        "    \n",
        "    # Identify categorical variables from the original dataset\n",
        "    cat_vars = df_original.select_dtypes(include=['object', 'category']).columns\n",
        "    \n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{applied_methods_vars} \\n\\n\")\n",
        "\n",
        "    for var in applied_methods_vars:\n",
        "        if var in cat_vars:  # For categorical variables, use bar plot\n",
        "            df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
        "            df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
        "            df_combined = pd.concat([df1, df2], axis=0)\n",
        "            \n",
        "            plt.figure(figsize=(15, 5))\n",
        "            sns.countplot(x=\"Value\", hue='Type', data=df_combined, palette=['#432371', '#FAAE7B'])\n",
        "            plt.title(f\"Bar Plot {flag_count}: {var}\")\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.legend()\n",
        "\n",
        "        else:  # For numerical variables, use KDE plot\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            sns.histplot(df_original[var].dropna(), color=\"#432371\", label='Original', alpha=0.5, edgecolor='black', zorder=1, kde=True)\n",
        "            sns.histplot(df_cleaned[var].dropna(), color=\"#FAAE7B\", label='Cleaned', alpha=0.5, edgecolor='black', zorder=1, kde=True)\n",
        "            #sns.kdeplot(df_original[var].dropna(), color=\"#432371\", label='Original', zorder=2)\n",
        "            #sns.kdeplot(df_cleaned[var].dropna(), color=\"#FAAE7B\", label='Cleaned', zorder=2)\n",
        "            plt.title(f\"KDE Plot {flag_count}: {var}\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        flag_count += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Exclusion of Specific Features: We have decided to remove the columns 'EnclosedPorch' and 'WoodDeckSF'. Despite their potential relevance in augmenting a property's size, their high rate of missing values (exceeding 80%) undermines their utility for predictive modeling. The lack of substantial data variation in these columns across different house price levels supports their exclusion from our analysis.\n",
        "\n",
        "* Approaches for Imputation:\n",
        "\n",
        "* a. Mean Imputation Application: For the columns 'LotFrontage' and 'BedroomAbvGr', we will apply mean imputation. This choice is grounded in the observation that their distribution patterns are relatively symmetrical, resembling a normal distribution. Substituting missing values with the mean will maintain the central tendency of these features.\n",
        "\n",
        "* b. Median Imputation Utilization: The features '2ndFlrSF', 'GarageYrBlt', and 'MasVnrArea' will undergo median imputation. Despite these features being somewhat normally distributed, the presence of skewed data warrants the use of the median. This approach is preferred over the mean, as it is less susceptible to distortion by outliers, providing a more robust measure of central tendency.\n",
        "\n",
        "* c. Categorical Imputation for Specific Features: For 'GarageFinish' and 'BsmtFinType1', categorical imputation is the chosen method. As these are categorical variables, employing a MeanMedian imputation approach is infeasible. Instead, we will replace missing values with the most frequently occurring category within each respective feature.\n",
        "\n",
        "* This tailored approach to handling missing data aligns with the specific characteristics of each variable in our dataset, enhancing the integrity and utility of the cleaned data for subsequent analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Set And Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['SalePrice'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_data_missing = AssessMissingValues(TrainSet)\n",
        "print(f\"* Total variables affected by missing data: {df_data_missing.shape[0]} \\n\")\n",
        "df_data_missing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop variables "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Variable Removal: Initially, the code specifies a list of variables, 'EnclosedPorch' and 'WoodDeckSF', which are identified for removal from the dataset. This is done in anticipation that dropping these variables, due to their high rate of missing data, will refine the dataset for better analysis and modeling.\n",
        "\n",
        "* Feature Dropping and Confirmation: The DropFeatures class from the feature_engine.selection module is then utilized to execute the removal of these specified variables from the training dataset (TrainSet). After this operation, the code conducts a verification check to ensure that each of the intended variables has indeed been dropped from the DataFrame. This verification is presented through a series of print statements that confirm the absence of each variable in the updated DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Applying feature removal process\n",
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "# Specifying the variables to be excluded\n",
        "drop_columns = ['EnclosedPorch', 'WoodDeckSF']\n",
        "feature_dropper = DropFeatures(features_to_drop=drop_columns)\n",
        "df_updated = feature_dropper.fit_transform(TrainSet)\n",
        "\n",
        "# Checking if the specified columns have been successfully removed\n",
        "for column in drop_columns:\n",
        "    print(f\"Is '{column}' present in the updated DataFrame? {'Yes' if column in df_updated.columns else 'No'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Applying Mean Imputation to specified variables\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "# Implementing Mean Imputation\n",
        "mean_impute_vars = ['LotFrontage', 'BedroomAbvGr']\n",
        "mean_imputer = MeanMedianImputer(imputation_method='mean', variables=mean_impute_vars)\n",
        "df_imputed = mean_imputer.fit_transform(TrainSet)\n",
        "\n",
        "# Visualizing the impact of mean imputation on the data\n",
        "VisualizeDataCleaningImpact(df_original=TrainSet, df_cleaned=df_imputed, applied_methods_vars=mean_impute_vars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Median Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* This code snippet demonstrates the process of median imputation for handling missing values in a dataset. It specifically targets three variables: '2ndFlrSF', 'GarageYrBlt', and 'MasVnrArea'. The MeanMedianImputer from the feature_engine.imputation library is utilized, configured for median imputation. After applying this imputation method to the TrainSet, the impact of this data cleaning step is visualized through the VisualizeDataCleaningImpact function. This function compares the original dataset with the cleaned version, highlighting changes in data distribution for the specified variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "# Specifying the variables for median imputation\n",
        "median_impute_vars = ['2ndFlrSF', 'GarageYrBlt', 'MasVnrArea']\n",
        "median_imputer = MeanMedianImputer(imputation_method='median', variables=median_impute_vars)\n",
        "df_median_imputed = median_imputer.fit_transform(TrainSet)\n",
        "\n",
        "# Visualizing the impact of median imputation on the data\n",
        "VisualizeDataCleaningImpact(df_original=TrainSet, df_cleaned=df_median_imputed, applied_methods_vars=median_impute_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet['GarageArea'] ==0)][['GarageYrBlt', 'GarageArea']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Shows where the var are 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "categorical_vars = ['GarageFinish', 'BsmtFinType1', 'BsmtExposure']\n",
        "imputer = CategoricalImputer(imputation_method='missing', fill_value='None', variables=categorical_vars)\n",
        "df_cat_imputation = imputer.fit_transform(TrainSet)\n",
        "VisualizeDataCleaningImpact(df_original=TrainSet,\n",
        "                            df_cleaned=df_cat_imputation,\n",
        "                            applied_methods_vars=categorical_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet['GarageArea'] ==0)][['GarageFinish', 'GarageArea']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Mean Imputation: This step uses the MeanMedianImputer class with imputation_method='mean' to impute missing values in the 'LotFrontage' and 'BedroomAbvGr' columns using their respective means. This is suitable for continuous variables where the mean is a good estimate of central tendency.\n",
        "\n",
        "* Median Imputation: Also utilizing the MeanMedianImputer class but with imputation_method='median', this step addresses missing values in the '2ndFlrSF' and 'MasVnrArea' columns by replacing them with the median of each column. This method is often used for skewed distributions or when the median is a more robust measure than the mean.\n",
        "\n",
        "* Categorical Imputation: The CategoricalImputer class, set to imputation_method='frequent', is used for the 'GarageFinish' and 'BsmtFinType1' columns. It replaces missing values with the most frequent category within each column. This approach is common for categorical variables where the mode (or most frequent category) is a logical choice for imputation.\n",
        "\n",
        "* Dropping Variables: The final step uses the DropFeatures class to remove specific columns from the dataset. In this case, 'EnclosedPorch', 'GarageYrBlt', and 'WoodDeckSF' are dropped. This step is crucial for eliminating features that are not useful for the analysis or modeling process.\n",
        "\n",
        "* Overall, this pipeline is a structured approach to clean and prepare the data for further analysis or modeling, enhancing the quality and utility of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "# Define a structured data preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('impute_mean', MeanMedianImputer(imputation_method='mean', \n",
        "                                      variables=['LotFrontage', 'BedroomAbvGr'])),\n",
        "    ('impute_median', MeanMedianImputer(imputation_method='median', \n",
        "                                        variables=['2ndFlrSF', 'MasVnrArea'])),\n",
        "    ('impute_categorical', CategoricalImputer(imputation_method='frequent', \n",
        "                                              variables=['GarageFinish', 'BsmtFinType1', 'BsmtExposure'])),\n",
        "    ('drop_features', DropFeatures(features_to_drop=['EnclosedPorch', 'GarageYrBlt', 'WoodDeckSF']))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* After this we apply the dataset to the whole dataset. We do it to get cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet, TestSet = preprocessing_pipeline.fit_transform(TrainSet) , preprocessing_pipeline.fit_transform(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = preprocessing_pipeline.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AssessMissingValues(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AssessMissingValues(TrainSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AssessMissingValues(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* By running this we see that there is no missing data to handle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We make an directory for the cleaned data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    os.makedirs(name='outputs/datasets/cleaned')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaned Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "file_path = f'outputs/ml_pipeline/data_cleaning'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=preprocessing_pipeline, filename=f\"{file_path}/preprocessing_pipeline.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
